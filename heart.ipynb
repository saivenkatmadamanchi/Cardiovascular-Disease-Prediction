{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset is taken for the UC Irvine Machine Learning Repository.  "
      ],
      "metadata": {
        "id": "aD-S92dzGWv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset info:-\n",
        "\n",
        "This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them.  In particular, the Cleveland database is the only one that has been used by ML researchers to date.  The \"goal\" field refers to the presence of heart disease in the patient.  It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).  \n",
        "   \n",
        "The names and social security numbers of the patients were recently removed from the database, replaced with dummy values.\n",
        "\n",
        "One file has been \"processed\", that one containing the Cleveland database.  All four unprocessed files also exist in this directory.\n",
        "\n",
        "To see Test Costs (donated by Peter Turney)"
      ],
      "metadata": {
        "id": "uWDc6aFKGtnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ucimlrepo"
      ],
      "metadata": {
        "id": "6Q5RlrVsIwl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imbalanced-learn"
      ],
      "metadata": {
        "id": "5qPk-mhQJOPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl7iTfjqcoyj"
      },
      "outputs": [],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "heart_disease = fetch_ucirepo(id=45) # fetch dataset\n",
        "\n",
        "X = heart_disease.data.features # data (as pandas dataframes)\n",
        "y = heart_disease.data.targets"
      ],
      "metadata": {
        "id": "aSG8SZaZdK1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge DataFrames based on index\n",
        "merged_df = pd.merge(X, y, left_index=True, right_index=True, how='inner')\n",
        "\n",
        "# Remove rows with NaN values\n",
        "merged_df = merged_df.dropna()\n",
        "\n",
        "# Assuming 'df' is your DataFrame and 'column_name' is the column for which you want to count distinct values\n",
        "value_counts = merged_df['num'].value_counts()\n",
        "\n",
        "# Display the counts\n",
        "print(\"Count per distinct value:\")\n",
        "print(value_counts)"
      ],
      "metadata": {
        "id": "cpfAASrpl59t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X should contain the features, and y should contain the target variable\n",
        "X = merged_df.drop('num', axis=1)\n",
        "y = merged_df['num']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use SMOTE to oversample the minority classes\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Combine the resampled data into a new DataFrame\n",
        "df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='num')], axis=1)\n",
        "\n",
        "# Check the distribution of the target variable in the resampled dataset\n",
        "print(df_resampled['num'].value_counts())"
      ],
      "metadata": {
        "id": "exMscm2qjNU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge DataFrames based on index\n",
        "merged_df = pd.merge(X, y, left_index=True, right_index=True, how='inner')\n",
        "\n",
        "# Remove rows with NaN values\n",
        "merged_df = merged_df.dropna()"
      ],
      "metadata": {
        "id": "zH6JXTxcf-fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'target_column' is the column name that represents your target variable\n",
        "target_column = 'target_column'\n",
        "\n",
        "# Split merged_df into X and y\n",
        "X = df_resampled.drop(columns=['num'])\n",
        "y = df_resampled['num']\n",
        "\n",
        "print(X.isnull().sum())"
      ],
      "metadata": {
        "id": "_ZWECeJ8gUzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Support Vector Machine classifier\n",
        "classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "pqTfnBl7eckV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "y_pred_dt = dt_classifier.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(f\"Decision Tree Accuracy: {accuracy_dt * 100:.2f}%\")\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")\n",
        "\n",
        "# Logistic Regression\n",
        "lr_classifier = LogisticRegression(random_state=42)\n",
        "lr_classifier.fit(X_train, y_train)\n",
        "y_pred_lr = lr_classifier.predict(X_test)\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_lr * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "g7NNzz1dhxG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Create the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Use the best model to make predictions on the test set\n",
        "y_pred_rf_optimized = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_rf_optimized = accuracy_score(y_test, y_pred_rf_optimized)\n",
        "print(f\"Optimized Random Forest Accuracy: {accuracy_rf_optimized * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "i6J1_L1kkg4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_gb = gb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "print(f\"Gradient Boosting Accuracy: {accuracy_gb * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "PS28g1tnFCv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "# Create a Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Create Grid Search CV\n",
        "grid_search = GridSearchCV(gb_classifier, param_grid, cv=3, scoring='accuracy')\n",
        "\n",
        "# Fit the model to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Make predictions on the test data using the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_gb = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the best model\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "print(f\"Gradient Boosting Accuracy: {accuracy_gb * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "6XwGLYiiFtgv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}